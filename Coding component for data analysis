#import necessary libraries
import pandas as pd
import numpy as np
import random
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from textblob import TextBlob
from nltk.sentiment import SentimentIntensityAnalyzer

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#loading the data
amazon_reviews_dataset = pd.read_csv('reviews.csv', quotechar='"')
print(amazon_reviews_dataset.head())

# Initialize the lemmatizer
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """
    Function to preprocess text by converting to lowercase, tokenizing,
    removing stopwords and punctuations, and lemmatizing.
    """
    # Convert text to lowercase
    text = str(text).lower()
    # Tokenize text
    tokens = word_tokenize(text)
    # Load stopwords only once to improve performance
    stop_words = set(stopwords.words('english'))
    # Remove stopwords and punctuations
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    # Lemmatize words
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(tokens)

def map_label(score):
    """
    Maps a numerical score to a category label.
    """
    if score <= 2:
        return 'negative'
    elif score == 3:
        return 'neutral'
    else:
        return 'positive'

# Apply preprocessing to the dataset
amazon_reviews_dataset['processed_text'] = amazon_reviews_dataset['Text'].apply(preprocess_text)

# Map scores to categories
amazon_reviews_dataset['true_category'] = amazon_reviews_dataset['Score'].apply(map_label)

# Function to return the polarity score using TextBlob
def get_textblob_sentiment(text):
    return TextBlob(text).sentiment.polarity

# Initialize SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Function to return the compound score using VADER
def get_vader_sentiment(text):
    return sia.polarity_scores(text)['compound']

# Function to apply sentiment analysis to a DataFrame
def apply_sentiment_analysis(df):
    df['TextBlob_Sentiment'] = df['processed_text'].apply(get_textblob_sentiment)
    df['VADER_Sentiment'] = df['processed_text'].apply(get_vader_sentiment)
    return df

# Classify TextBlob scores into categories
def classify_textblob(score):
    if score <= -0.1:
        return 'negative'
    elif score <= 0.1:
        return 'neutral'
    else:
        return 'positive'

# Classify VADER scores into categories
def classify_vader(score):
    if score <= -0.05:
        return 'negative'
    elif score <= 0.05:
        return 'neutral'
    else:
        return 'positive'

# Apply sentiment analysis and categorization
amazon_reviews_sentiment = apply_sentiment_analysis(amazon_reviews_dataset)
amazon_reviews_sentiment['TextBlob_category'] = amazon_reviews_sentiment['TextBlob_Sentiment'].apply(classify_textblob)
amazon_reviews_sentiment['VADER_category'] = amazon_reviews_sentiment['VADER_Sentiment'].apply(classify_vader)

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

# Evaluation metrics
def calculate_accuracy_and_report(df, true_col, pred_col, model_name):
    accuracy = accuracy_score(df[true_col], df[pred_col])
    report = classification_report(df[true_col], df[pred_col], target_names=['negative', 'neutral', 'positive'])
    print(f"Accuracy for {model_name}: {accuracy}")
    print(f"\nClassification Report for {model_name}:\n{report}")

calculate_accuracy_and_report(amazon_reviews_sentiment, 'true_category', 'TextBlob_category', 'TextBlob')
calculate_accuracy_and_report(amazon_reviews_sentiment, 'true_category', 'VADER_category', 'VADER')

# Function to plot confusion matrix
def plot_confusion_matrix(df, true_col, pred_col, model_name):
    cm = confusion_matrix(df[true_col], df[pred_col])
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Labels')
    plt.xlabel('Predicted Labels')
    plt.show()

# Plotting confusion matrices
plot_confusion_matrix(amazon_reviews_sentiment, 'true_category', 'TextBlob_category', 'TextBlob')
plot_confusion_matrix(amazon_reviews_sentiment, 'true_category', 'VADER_category', 'VADER')

# Set random seeds for reproducibility
np.random.seed(42)
random.seed(42)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    amazon_reviews_dataset['processed_text'], 
    amazon_reviews_dataset['Score'].apply(map_label), 
    test_size=0.2, 
    random_state=42
)

# Vectorize the text data
vectorizer = CountVectorizer()
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# Define parameter grids for GridSearchCV
nb_params = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}  # Smoothing parameters for Naive Bayes
rf_params = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [10, 50, 100, None]  # Maximum depth of the trees
}

# Initialize GridSearchCV for Naive Bayes and Random Forest with 'f1_weighted' scoring
nb_grid = GridSearchCV(MultinomialNB(), nb_params, cv=5, scoring='f1_weighted')
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='f1_weighted')

# Fit models
nb_grid.fit(X_train_vect, y_train)
rf_grid.fit(X_train_vect, y_train)

# Predict using the best models
nb_predictions = nb_grid.best_estimator_.predict(X_test_vect)
rf_predictions = rf_grid.best_estimator_.predict(X_test_vect)

# Generate and print classification reports
print("Classification Report for Naive Bayes:")
print(classification_report(y_test, nb_predictions, target_names=['negative', 'neutral', 'positive']))

print("\nClassification Report for Random Forest:")
print(classification_report(y_test, rf_predictions, target_names=['negative', 'neutral', 'positive']))

# Confusion matrices
nb_conf_matrix = confusion_matrix(y_test, nb_predictions)
rf_conf_matrix = confusion_matrix(y_test, rf_predictions)

# Function to plot confusion matrix
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
    plt.title(title)
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

# Plotting confusion matrices
plot_confusion_matrix(nb_conf_matrix, title='Naive Bayes Confusion Matrix')
plot_confusion_matrix(rf_conf_matrix, title='Random Forest Confusion Matrix')


